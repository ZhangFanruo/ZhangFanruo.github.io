<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="generator" content="Hugo 0.66.0" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<link rel="stylesheet" href="../css/normalize.css">
<link rel="stylesheet" href="../css/skeleton.css">
<link rel="stylesheet" href="../css/custom.css">
<link rel="alternate" href="index.xml" type="application/rss+xml" title="Speech Research">
<link rel="shortcut icon" href="favicon.png" type="image/x-icon" />
<title>MultiSpeech: Multi-Speaker Text to Speech with Transformer - Speech Research</title>
</head>
<body>

<div class="container">

	<header role="banner">
		
			
		
		
	</header>


	<main role="main">
		<article itemscope itemtype="https://schema.org/BlogPosting">
            <h1 class="entry-title" itemprop="headline">MultiSpeech: Multi-Speaker Text to Speech with Transformer</h1>
			
			<section itemprop="entry-text">
				<!-- <br>
<img src="../audio/lrspeech/fig/2.png"  width="90%" height="40px"  alt="LRSpeech_pipline" />
<br>
<img src="../audio/lrspeech/fig/3.png"  width="90%" height="40px" alt="LRSpeech_model" /> -->
<!-- Paper: <a href="../papers/fastspeech_2019.pdf">FastSpeech: Fast, Robust and Controllable Text to Speech</a> -->
<!-- ArXiv: [arXiv:1905.09263](https://arxiv.org/abs/1905.09263) -->
<!-- Reddit Discussions: [Click me](https://www.reddit.com/r/MachineLearning/comments/brzwi5/r_fastspeech_fast_robust_and_controllable_text_to/) -->
<h2 id="authors">Authors</h2>
<ul>
<li>Mingjian Chen (Perking University) <a href="mailto:milk@pku.edu.cn">milk@pku.edu.cn</a></li>
<li>Xu Tan (Microsoft Research) <a href="mailto:xuta@microsoft.com">xuta@microsoft.com</a></li>
<li>Yi Ren (Zhejiang University) <a href="mailto:rayeren@zju.edu.cn">rayeren@zju.edu.cn</a></li>
<li>Jin Xu (Tsinghua University) <a href="mailto:j-xu18@mails.tsinghua.edu.cn">j-xu18@mails.tsinghua.edu.cn</a></li>
<li>Hao Sun (Perking University) <a href="mailto:sigmeta@pku.edu.cn">sigmeta@pku.edu.cn</a></li>
<li>Sheng Zhao (Microsoft STC Asia) <a href="mailto:Sheng.Zhao@microsoft.com">Sheng.Zhao@microsoft.com</a></li>
<li>Tao Qin (Microsoft Research) <a href="mailto:taoqin@microsoft.com">taoqin@microsoft.com</a></li>
<li>Tie-Yan Liu (Microsoft Research) <a href="mailto:tyliu@microsoft.com">tyliu@microsoft.com</a></li>
</ul>
<!-- ## Abstract

Speech synthesis (text to speech, TTS) and recognition (automatic speech recognition, ASR) are important speech tasks, and require a large amount of text and speech pairs for model training. However, there are more than 6,000 languages in the world and most languages are lack of speech training data, which poses significant challenges when building TTS and ASR systems for extremely low-resource languages. In this paper, we develop LRSpeech, a TTS and ASR system under the extremely low-resource setting, which can support rare languages with low data cost. LRSpeech consists of three key techniques: 1) pre-training on rich-resource languages and fine-tuning on low-resource languages; 2) dual transformation between TTS and ASR to iteratively boost the accuracy of each other; 3) knowledge distillation to customize the TTS model on a high-quality target-speaker voice and improve the ASR model on multiple voices. We conduct experiments on an experimental language (English) and a truly low-resource language (Lithuanian) to verify the effectiveness of LRSpeech. Experimental results show that LRSpeech 1) achieves high quality for TTS in terms of both intelligibility (more than $98\%$ intelligibility rate) and naturalness (above 3.5 mean opinion score (MOS)) of the synthesized speech, which satisfy the requirements for industrial deployment, 2) achieves promising recognition accuracy for ASR, and 3) last but not least, uses extremely low-resource training data. We also conduct comprehensive analyses on LRSpeech with different amounts of data resources, and provide valuable insights and guidances for industrial deployment. We are currently deploying LRSpeech into a commercialized cloud speech service to support TTS on more rare languages.  -->
<h2 id="tts-audio-samples-in-the-paper">TTS Audio Samples in the Paper</h2>
<h3 id="experiments-on-vctk-and-libritts">Experiments on VCTK and LibriTTS</h3>
<!-- 288 2836 -->
<p><em>VCTK speaker : Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.</em></p>
<table><thead>
<tr>
<th style="text-align: center">GT</th>
<th style="text-align: center">GT mel + Wavenet</th>
<th style="text-align: center">Transformer based TTS</th>
<th style="text-align: center">MultiSpeech</th>
</tr></thead><tbody>
<tr>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/base/vctk/228/gt/0000000001.wav" autoplay/>Your browser does not support the audio element.</audio></td>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/base/vctk/228/gt mel vocoder/0000000001.wav" autoplay/>Your browser does not support the audio element.</audio></td>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/base/vctk/228/transformertts/0000000001.wav" autoplay/>Your browser does not support the audio element.</audio></td>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/base/vctk/228/multispeech/0000000001.wav" autoplay/>Your browser does not support the audio element.</audio></td>
</tr>
</tbody></table>
<p><em>LibriTTS speaker : The hectic flushed into her thin cheeks, but her voice sounded calm as before.</em></p>
<table><thead>
<tr>
<th style="text-align: center">GT</th>
<th style="text-align: center">GT mel + Wavenet</th>
<th style="text-align: center">Transformer based TTS</th>
<th style="text-align: center">MultiSpeech</th>
</tr></thead><tbody>
<tr>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/base/libritts/2836/gt/0000000001.wav" autoplay/>Your browser does not support the audio element.</audio></td>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/base/libritts/2836/gt mel vocoder/0000000001.wav" autoplay/>Your browser does not support the audio element.</audio></td>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/base/libritts/2836/transformertts/0000000001.wav" autoplay/>Your browser does not support the audio element.</audio></td>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/base/libritts/2836/multispeech/0000000001.wav" autoplay/>Your browser does not support the audio element.</audio></td>
</tr>
</tbody></table>
<h3 id="audios-of-ablation-study-on-vctk">Audios of ablation study on VCTK</h3>
<p><em>VCTK speaker : Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.</em></p>
<table><thead>
<tr>
<th style="text-align: center">MultiSpeech</th>
<th style="text-align: center">-DC</th>
<th style="text-align: center">-LN</th>
<th style="text-align: center">-PB</th>
</tr></thead><tbody>
<tr>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/ablation/vctk/228/multispeech/0000000001.wav" autoplay/>Your browser does not support the audio element.</audio></td>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/ablation/vctk/228/-dc/0000000001.wav" autoplay/>Your browser does not support the audio element.</audio></td>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/ablation/vctk/228/-ln/0000000001.wav" autoplay/>Your browser does not support the audio element.</audio></td>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/ablation/vctk/228/-pb/0000000001.wav" autoplay/>Your browser does not support the audio element.</audio></td>
</tr>
</tbody></table>
<table><thead>
<tr>
<th style="text-align: center">-DC-LN-PB</th>
</tr></thead><tbody>
<tr>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/ablation/vctk/228/transformertts/0000000001.wav" autoplay/>Your browser does not support the audio element.</audio></td>
</tr>
</tbody></table>
<h3 id="audios-of-fastspeech-multispeech-as-teacher-model">Audios of FastSpeech (MultiSpeech as teacher model).</h3>
<p><em>VCTK speaker : People look, but no one ever finds it.</em></p>
<table><thead>
<tr>
<th style="text-align: center">GT</th>
<th style="text-align: center">MultiSpeech</th>
<th style="text-align: center">FastSeech</th>
</tr></thead><tbody>
<tr>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/fastspeech/6/gt/0000000010.wav" autoplay/>Your browser does not support the audio element.</audio></td>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/fastspeech/6/teacher/0000000010.wav" autoplay/>Your browser does not support the audio element.</audio></td>
<td style="text-align: center"><audio controls="controls" ><source src="../audio/multispeech/fastspeech/6/student/0000000010.wav" autoplay/>Your browser does not support the audio element.</audio></td>
</tr>
</tbody></table>
<h2 id="our-related-works">Our Related Works</h2>
<p><a href="/unsuper/">Almost Unsupervised Text to Speech and Automatic Speech Recognition</a><br>
<a href="/fastspeech/">FastSpeech: Fast, Robust and Controllable Text to Speech</a><br>
<a href="/seminas/">Semi-Supervised Neural Architecture Search</a><br>
<a href="/lrspeech/">LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition</a><br>
<a href="/fastspeech2/">FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech</a><br>
<a href="/uwspeech/">UWSpeech: Speech to Speech Translation for Unwritten Languages</a><br>
<a href="/denoispeech/">Denoising Text to Speech with Frame-Level Noise Modeling</a><br></p>

			</section>
		</article>
	</main>


	

</div>

<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-139981676-1', 'auto');
	ga('send', 'pageview');
</script>

<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
         HTML: ["input/TeX","output/HTML-CSS"],
         TeX: {
                Macros: {
                         bm: ["\\boldsymbol{#1}", 1],
                         argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                         argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
                extensions: ["AMSmath.js","AMSsymbols.js"],
                equationNumbers: { autoNumber: "AMS" } },
         extensions: ["tex2jax.js"],
         jax: ["input/TeX","output/HTML-CSS"],
         tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true },
         "HTML-CSS": { availableFonts: ["TeX"],
                       linebreaks: { automatic: true } }
     });
 </script>

 <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
       }
     });
 </script>

 <script type="text/javascript" async
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
 </script>




</body>
</html>

